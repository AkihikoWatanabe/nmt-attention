This is a implementation of one of the Neural Machine Translation model with Attention.
See the paper: https://arxiv.org/abs/1409.0473

This implementation uses LSTM as gated hidden unit instead of GRU.

Environment:
* Anaconda
* Chainer
