This is a implementation of the Neural Machine Translation model with Global Attention.
See the paper: https://arxiv.org/abs/1409.0473

This implementation uses LSTM as gated hidden unit instead of GRU.

This implementation is based on the https://github.com/odashi/chainer_examples.
Thank you.

Environment:
* python 2.7
* Anaconda
* Chainer 1.8.2
